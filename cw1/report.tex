\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage{titling}
\usepackage{listings}
\usepackage{subfig}

\graphicspath{{figures/}}
\title{\vspace{-2cm}4F13 Coursework 1 - Gaussian Processes}
\preauthor{}
\author{}
\postauthor{}
\date{October 2024}

\begin{document}

\maketitle
\section{Task A}

\begin{lstlisting}[caption=Code to train hyper-parameters and generate the predictive distribution of a GP with squared exponential covariance, captionpos=b, basicstyle=\small]
meanfunc = []; hyp_init.mean = [];
covfunc = @covSEiso; hyp_init.cov = [-1 0];
likfunc = @likGauss; hyp_init.lik = 0;
hyp_opt = minimize(hyp_init, @gp, -100, @infGaussLik, 
                   meanfunc, covfunc, likfunc, x, y);
[mu, s2] = gp(hyp_opt, @infGaussLik, meanfunc, covfunc, likfunc, x, y, xs);
\end{lstlisting}
\label{lst:A}

Our model has a GP with squared exponential (SE) covariance function and zero mean along with a gaussian noise. The SE covariance function has two hyper-parameters, $\lambda$ which controls the characteristic length scale of the process and $\sigma_f^2$ which controls the variance in the output value of the GP. The gaussian noise has one hyper-parameter, $\sigma_n^2$, the variance of our measurement noise. 
\[y = f(x) + \eta : \eta \sim \mathcal{N}(0, \sigma_n^2)\]
\[f = \mathcal{N}(0, k(x, x'))\]
\[k(x,x') = \sigma_f^2 \exp(-\frac{(x-x')'(x-x')}{2\lambda^2})\]

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & $\lambda$ & $\sigma_f$ & $\sigma_n$ & $\ln(Z_{|\textbf{y}})$ \\
        \hline
        Initial & $0.3679$ & $1$ & $1$ & $\num{-9.2910e+01}$ \\ 
        Optimised & $0.1282$ & $0.8970$ & $0.1178$ & $\num{-1.1899e+01}$ \\ 
        \hline
    \end{tabular}
    \caption{Initial and optimum hyper-parameter values when maximising log marginal likelihood of data}
    \label{table:A_hyper_parameters}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{A/initial_fit_plot}
    \caption{Mean and 95\% prediction interval of GP with squared exponential covariance function, optimised hyper-parameters used given in Table \ref{table:A_hyper_parameters}}
    \label{fig:A}
\end{figure}

When we train our model hyper parameters to maximise log marginal likelihood we achieve the optimum values shown in Table \ref{table:A_hyper_parameters}. The covariance length scale, $\lambda$ is similar to the length scale of the undulations in our data. The $\sigma_f$ is much larger than $\sigma_n$ this makes sense as most of the variation in our data can be explained by our GP instead of the measurement noise.

We plot the mean and 95\% error bars of the predictive distribution (Equ.\ref{equation:A_predictive_distribution}) in Fig \ref{fig:A}. We can see that the predictive error bars are smaller where we have data and return to the prior variance ($k(x, x) = \sigma_f^2$) plus $\sigma_n^2$ when there is no nearby data. This is clear from the form of the predictive distribution covariance, the first term is the prior covariance, the second is the noise variance and the third reduces the variance based on the distance between the sample point and our data.

\begin{equation}
    \begin{gathered}
        y_* \sim \mathcal{N}(m_{|\textbf{y}}(x_*), k_{|\textbf{y}}(x_*, x_*)) \\
        m_{|\textbf{y}}(x) = m(x) + k(x, \textbf{x})[k(\textbf{x}, \textbf{x}) + \sigma_n^2 I]^{-1} (\textbf{y} - m(\textbf{x})) \\
        k_{|\textbf{y}}(x, x') = k(x, x') + \sigma_n^2 - k(x, \textbf{x})[k(\textbf{x}, \textbf{x}) + \sigma_n^2 I]^{-1} k(\textbf{x}, x')
    \end{gathered}
    \label{equation:A_predictive_distribution}
\end{equation}

\section{Task B}

\begin{lstlisting}[caption=Code to sweep $\lambda$ and $\sigma_n$ to identify local minima of $\log(Z_{|\textbf{y}})$, captionpos=b, basicstyle=\small]
sf = 0;
[ell, sn] = meshgrid(-4:0.1:4, -4:0.1:1);
nlZ = zeros(size(ell));
for i = 1:numel(ell)
    hyp = struct('mean', [], 'cov', [ell(i) sf], 'lik', sn(i));
    nlZ(i) = gp(hyp, @infGaussLik, meanfunc, covfunc, likfunc, x, y);
end
minima = islocalmin2(nlZ);
\end{lstlisting}
\label{lst:B}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{B/model_evidence_contour.eps}
    \caption{Contour of $\ln(\ln(Z_{|\textbf{y}}))$ with $\lambda$ and $\sigma_n$ for fixed $\sigma_f = 1$. Shows the two local maxima of marginal likelihood given in Table \ref{table:B_hyper_parameter_optima}}
    \label{fig:B_marginal_liklihood_contour}
\end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & $\lambda$ & $\sigma_f$ & $\sigma_n$ & $\log(Z_{|\textbf{y}})$ \\
        \hline
        Optimum 1 & $0.1282$ & $0.8970$ & $0.1178$ & $\num{-1.1899e+01}$ \\ 
        Optimum 2 & $8.0492$ & $0.6957$ & $0.6631$ & $\num{-7.8220e+01}$ \\ 
        \hline
    \end{tabular}
    \caption{Hyper-parameter values at 2 local maxima of log marginal likelihood}
    \label{table:B_hyper_parameter_optima}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{B/hp_optimum_comparison}
    \caption{Prediction intervals of GP with hyper-parameters from Table \ref{table:B_hyper_parameter_optima}. Top: Optimum 1, Bottom: Optimum 2}
    \label{fig:B_hyper_parameter_optimum_comparison}
\end{figure}

\section{Task C}
\begin{lstlisting}[caption=Code to use periodic SE covariance. Training and prediction code same as Listing \ref{lst:A}, captionpos=b, basicstyle=\small]
covfunc = @covPeriodic; hyp_init.cov = [-1 0 0];
\end{lstlisting}
\label{lst:C}


\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $\lambda$ & $p$ & $\sigma_f$ & $\sigma_n$ & $\log(Z_{|\textbf{y}})$ \\
        \hline
        $0.7048$ & $0.9991$ & $0.6941$ & $0.0851$ & $\num{2.9300e+01}$ \\ 
        \hline
    \end{tabular}
    \caption{Hyper-parameter values for periodic SE covariance function}
    \label{table:C_periodic_covariance_hyper_parameters}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{C/periodic_covariance_plot}
    \caption{Prediction intervals for periodic SE covariance function with hyper parameters given in Table \ref{table:C_periodic_covariance_hyper_parameters}}
    \label{fig:B_periodic_covariance_prediction_intervals}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{C/noise_cdf} % Replace with your figure file
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{C/noise_plot} % Replace with your figure file
    \end{minipage}
    \caption{Caption for figure 2}
    \label{fig:C_noise_distribution}
\end{figure}

\section{Task D}

\begin{lstlisting}[caption=Code sample a GP with covariance defined by the product of a periodic SE kernel and a SE kernel, captionpos=b, basicstyle=\small]
N_points = 200; N_samples = 3;
x = linspace(-5,5,N_points)';
cov_func = {@covProd, {@covPeriodic, @covSEiso}}; hyp.cov = [-0.5 0 0 2 0];
K = feval(cov_func{:}, hyp.cov, x);
y = chol(K + 1e-6*eye(N_points))' * gpml_randn(2, N_points, N_samples);
\end{lstlisting}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $p^1$ & $\lambda^1$ & $\sigma_f^1$ & $\lambda^2$ & $\sigma_f^2$ \\
        \hline
        $1$ & $0.6065$ & $1$ & $7.3891$ & $1$ \\
        \hline
    \end{tabular}
    \caption{Hyper-parameter values for periodic SE covariance function}
    \label{table:D_product_covariance_hyper_parameters}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{D/prod_kernel_samples}
    \caption{Samples from 3 covariance kernels, hyper-parameters for each given in Table}
    \label{fig:D_prod_kernel_samples}
\end{figure}

\end{document}