\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{titling}
\usepackage{listings}
\usepackage{float}
\usepackage{subcaption}
\usepackage{pdfpages}


\newcommand{\Lik}{\mathcal{L}}

\graphicspath{{figures/}}
\title{\vspace{-2cm}4F13 Coursework 1 - Gaussian Processes}
\preauthor{}
\author{}
\postauthor{}
\date{November 2024}

\begin{document}

\includepdf[pages={1}]{coversheet}

\maketitle
\section{Task A}

Our model is defined in Equation \ref{eq:A_model}, we have a gaussian process ($f$) with a squared exponential (SE) covariance function , Equation \ref{eq:SE}, zero mean and gaussian likelihood. 

\begin{equation}
    y = f(x) + \eta : f \sim \mathcal{N}(0, k_{SE}(x, x')); \eta \sim \mathcal{N}(0, \sigma_n^2)
    \label{eq:A_model}
\end{equation}
\begin{equation}
    k_{SE}(x,x') = \sigma_f^2 \exp(-\frac{(x-x')^2}{2\lambda^2})    \label{eq:SE}
\end{equation}

The model hyper-parameters are trained by minimising the negative log marginal likelihood ($\Lik$). We do this and generate the predictive distribution using the code in Listing \ref{lst:A}. 

\begin{lstlisting}[caption=Code to train hyper-parameters and generate the predictive distribution of a GP with squared exponential covariance, label=lst:A, captionpos=b, basicstyle=\small, frame=tlrb]
meanf = []; covf = @covSEiso; likf = @likGauss; 
hyp_init.mean = []; hyp_init.cov = [-1 0]; hyp_init.lik = 0;
hyp_opt = minimize(hyp_init,@gp,-100,@infGaussLik,meanf,covf,likf,x,y);
[mu, s2] = gp(hyp_opt, @infGaussLik, meanf, covf, likf, x, y, xs);
\end{lstlisting}

The trained hyper-parameters are listed as Optimum 1 in Table \ref{table:AB_hyper_parameters}. We plot the data and predictive distribution in Figure \ref{fig:AB_predictive_distributions_1}. The hyper-parameters have the following interpretation; $\lambda$ - length scale, $\sigma_f$ - scale factor and $\sigma_n^2$ - measurement noise variance.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & $\lambda$ & $\sigma_f$ & $\sigma_n$ & $\Lik$ \\
        \hline
        Optimum 1 & $0.128$ & $0.897$ & $0.118$ & $\num{1.19e+01}$ \\ 
        Optimum 2 & $8.049$ & $0.696$ & $0.663$ & $\num{7.82e+01}$ \\ 
        \hline
    \end{tabular}
    \caption{Hyper-parameter values at 2 local minima of $\Lik$}
    \label{table:AB_hyper_parameters}
\end{table}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{B/hp_optimum_comparison_1} 
        \subcaption{Optimum 1}
        \label{fig:AB_predictive_distributions_1}
    \end{minipage}%
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{B/hp_optimum_comparison_2} 
        \subcaption{Optimum 2}
        \label{fig:AB_predictive_distributions_2}
    \end{minipage}
    \caption{Predictive mean and 95\% error bars of models with hyper-parameters given in Table \ref{table:AB_hyper_parameters}. Training data is plotted as well.}
    \label{fig:AB_predictive_distributions}
\end{figure}

From Figure \ref{fig:AB_predictive_distributions_1} we see that in regions of higher data density the error bars are small while where data is sparse they become wider and approach a constant value. This is explained by the form of the predictive variance, Equation \ref{eq:predictive_dist}. Our data inputs are denoted $\textbf{x}$. (When calculating the predictive error bars we evaluate our predictive covariance with $x' = x$, Equation \ref{eq:predictive_dist} gives the simplified equation in this case.)

\begin{equation}
    k_{|\textbf{y}}(x) = \sigma_f^2 + \sigma_n^2 - k_{SE}(x, \textbf{x})[k_{SE}(\textbf{x}, \textbf{x}) + \sigma_n^2 I]^{-1} k_{SE}(\textbf{x}, x)
    \label{eq:predictive_dist}
\end{equation}

There are 3 terms, the first two are constant, $\sigma_f^2 + \sigma_n^2$, these are the prior variance. $k_{SE}(\textbf{x}, \textbf{x}) + \sigma_n^2 I$ is positive definite by definition of the covariance kernel therefore the third term is always negative. This third term becomes larger in magnitude when there are many data points within $\lambda$ of $x$, this decreases the predictive variance where there is a higher density of data. This form makes intuitive sense too, we can be more confident in predictions where we have more data and where we have no data we can only use our prior knowledge.

\section{Task B}

To identify all local minima of $\Lik$ we perform a grid search over the hyper-parameters. Figure \ref{fig:B_marginal_liklihood_contour} shows a contour $\Lik$ for a slice of the search with $\sigma_f=1$ which shows the two minima well. The second optimum (Optimum 2 in Table \ref{table:AB_hyper_parameters}) has a much longer length scale, $\lambda$, and larger measurement noise, $\sigma_n$. From the predictive distribution, Figure \ref{fig:AB_predictive_distributions_2}, we see that this optimum results in a model that explains most of the output variation as measurement noise instead of the value of the function unlike Optimum 1 which does the opposite. However, this second optimum is a worse fit with a higher value of $\Lik$. Furthermore, by observing the distribution of the residuals we see that they do not seem to be independent of the input variable, while our model expects independent measurement noise. Therefore, we can conclude that Optimum 1 is more likely to be the model that generated this data. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\columnwidth]{B/model_evidence_contour.eps}
    \caption{Contour of $\ln(\Lik)$ with $\lambda$ and $\sigma_n$ for fixed $\sigma_f = 1$. Shows the two local minima of $\Lik$ given in Table \ref{table:AB_hyper_parameters}. We are plotting $\ln(\Lik)$ for more evenly spaced contours.}
    \label{fig:B_marginal_liklihood_contour}
\end{figure}

\section{Task C}

\begin{lstlisting}[caption=Code to use periodic SE covariance. Training and prediction code same as Listing \ref{lst:A}, label=lst:C, captionpos=b, basicstyle=\small, frame=tlrb]
covf = @covPeriodic; hyp_init.cov = [-1 0 0];
\end{lstlisting}

Equation \ref{eq:PSE} gives the form of the periodic squared exponential covariance function. It has a very similar form to the standard squared exponential covariance function, but the measure of "distance" between two points in input space is now $\sin(\frac{\pi}{p}(x-x'))$. This means that the "distance" between two points any multiple of $p$ apart is now zero, giving large covariance between these points. This means that samples from this GP will be periodic with period $p$. The optimised hyper-parameters for the periodic SE covariance function are given in Table \ref{table:C_periodic_covariance_hyper_parameters} and the prediction intervals are shown in Figure \ref{fig:C_periodic_covariance_prediction_intervals}.

\begin{equation}
    k_{PSE}(x,x') = \sigma_f^2 \exp(-\frac{2}{\lambda^2}\sin^2(\frac{\pi}{p}(x-x')))
    \label{eq:PSE}
\end{equation}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $\lambda$ & $p$ & $\sigma_f$ & $\sigma_n$ & $\Lik$ \\
        \hline
        $0.705$ & $0.999$ & $0.694$ & $0.085$ & $\num{-2.93e+01}$ \\ 
        \hline
    \end{tabular}
    \caption{Hyper-parameter values for periodic SE covariance function}
    \label{table:C_periodic_covariance_hyper_parameters}
\end{table}

The effect of the periodic covariance is clear in the prediction intervals. Unlike the previous model where prediction intervals were large in where the density of data was lower, now, as long as there is data a multiple of the period apart, the model has small prediction intervals. This is due to the form of the covariance function and predictive distribution.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\columnwidth]{C/periodic_covariance_plot}
    \caption{Prediction intervals for periodic SE covariance function with hyper-parameters given in Table \ref{table:C_periodic_covariance_hyper_parameters}}
    \label{fig:C_periodic_covariance_prediction_intervals}
\end{figure}

The marginal likelihood of the periodic model is higher than that of the standard squared exponential model, indicating a better fit. We also gain confidence in this model as our training data includes multiple periods. Further evidence that the periodic model is accurate can be seen in the residuals of the data, which are close normally distributed, Figure \ref{fig:C_residual_CDF}, and independent of the input variable, Figure \ref{fig:C_residuals}. This matches our model definition.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{C/noise_cdf}
        \subcaption{CDF of residuals}
        \label{fig:C_residual_CDF}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{C/noise_plot}
        \subcaption{Residuals}
        \label{fig:C_residuals}
    \end{minipage}
    \caption{Residuals of data when fitted with periodic SE covariance GP}
\end{figure}

\section{Task D}

\begin{lstlisting}[caption=Code to generate samples from a GP with covariance given by covf, label=lst:D, captionpos=b, basicstyle=\small, frame=tlrb]
N_points = 200; N_samples = 2;
x = linspace(-5,5,N_points)';
K = feval(covf{:}, hyp.cov, x);
y = chol(K + 1e-6*eye(N_points))' * gpml_randn(2, N_points, N_samples);
\end{lstlisting}

To sample from a GP we can choose a set of evaluation points - $\textbf{x}$ and evaluate our mean and covariance functions at these points to obtain a mean vector - $\boldsymbol{\mu}$ and covariance matrix - $K$. We can generate our samples $\textbf{y} = \boldsymbol{\mu} + chol(K)^T \boldsymbol{\eta}$ where $\boldsymbol{\eta} \sim \mathcal{N}(\mathbf{0}, I)$.

We require the Cholesky decomposition decomposition of $K$ - $chol(K)$, however, this is only possible for positive definite matrices. Our covariance kernel is positive semi-definite, therefore, to ensure $K$ is positive definite we can add $\epsilon I$ before the decomposition. This ensures it is positive definite without changing the behaviour of the matrix much. The code to generate the samples is shown in Listing \ref{lst:D} for $\epsilon = \num{1e-6}$.

We sample a GP with a covariance kernel that is the product of a long length scale SE kernel and short length scale periodic SE kernel. The form of the kernel is given in Equation \ref{eq:D_kernel} and hyper-parameter values are given in Table \ref{table:D_product_covariance_hyper_parameters}.

\begin{equation}
    k(x, x') = k_{PSE | p^{(1)}, \lambda^{(1)}, \sigma_f^{(1)}}(x, x')k_{SE | \lambda^{(2)}, \sigma_f^{(2)}}(x, x')
    \label{eq:D_kernel}
\end{equation}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $p^{(1)}$ & $\lambda^{(1)}$ & $\sigma_f^{(1)}$ & $\lambda^{(2)}$ & $\sigma_f^{(2)}$ \\
        \hline
        $1$ & $0.607$ & $1$ & $7.389$ & $1$ \\
        \hline
    \end{tabular}
    \caption{Hyper-parameter values for periodic SE covariance function}
    \label{table:D_product_covariance_hyper_parameters}
\end{table}

We observe that the samples in Figure \ref{fig:D_prod_kernel_samples_3} exhibit characteristics of both kernels shown in Figures \ref{fig:D_prod_kernel_samples_1} and \ref{fig:D_prod_kernel_samples_2}. Specifically, they display periodic behaviour with a period of $p^{(1)} = 1$ over short length scales, like Figure \ref{fig:D_prod_kernel_samples_1}, combined with gradual changes over larger length scales, Figure \ref{fig:D_prod_kernel_samples_2}.

In comparison to a purely periodic model, the prediction intervals of this model will not remain narrow even very far from any data. They will instead slowly decay with the length scale of the SE component. This strikes a middle ground between the models investigated in Task A and C where we want to gain prediction accuracy using the periodic behaviour while not making narrow predictions very far outside of our training data.


\begin{figure}[h]
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{D/prod_kernel_samples_1} 
        \subcaption{Periodic squared exponential kernel with small $\lambda$}
        \label{fig:D_prod_kernel_samples_1}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{D/prod_kernel_samples_2} 
        \subcaption{Squared exponential kernel with large $\lambda$}
        \label{fig:D_prod_kernel_samples_2}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{D/prod_kernel_samples_3} 
        \subcaption{Product kernel}
        \label{fig:D_prod_kernel_samples_3}
    \end{minipage}

    \caption{Samples from 3 covariance kernels, hyper-parameters for each given in Table \ref{table:D_product_covariance_hyper_parameters}}
    \label{fig:D_prod_kernel_samples}
\end{figure}

\section{Task E}

The squared exponential Automatic Relevance Determination (SE-ARD) kernel, given in Equation \ref{eq:SEARD}, is similar to the standard squared exponential kernel, but the distance measure can now be weighted differently for each input dimension. This can be useful when the input dimensions have different units or scales. 

\begin{equation}
    k_{SE-ARD}(x, x') = \sigma_f^2 \exp(-\frac{1}{2}\sum_{i=1}^{D} \frac{(x_i - x_i')^2}{\lambda_i}^2)
    \label{eq:SEARD}
\end{equation}

When we fit the sample data with the SE-ARD kernel we get the hyper-parameters given in Table \ref{table:E_hyper_parameters} case A. The length scales in each input dimension are similar so the model is not making use of the ARD property. The prediction intervals of the model are shown in Figure \ref{fig:E_kernel_compare_1}. 

\begin{table*}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        Case & $\sigma_f^1$ & $\lambda_1^1$ & $\lambda_2^1$ & $\sigma_f^1$ & $\lambda_1^1$ & $\lambda_2^1$ & $\sigma_n$ & $\Lik$ \\
        \hline
        A: $k_{SE-ARD}$ & $1.107$ & $1.511$ & $1.285$ & - & - & - & $0.1026$ & $\num{-1.9218e+01}$ \\ 
        B: $k_{SE-ARD}^{(1)} + k_{SE-ARD}^{(2)}$ & $0.7116$ & $1104$ & $0.9864$ & $1.108$ & $1.446$ & $1281$ & $0.0979$ & $\num{-6.6394e+01}$ \\
        \hline
    \end{tabular}
    \caption{Hyper-parameter values for periodic SE covariance function}
    \label{table:E_hyper_parameters}
\end{table*}

When our kernel is the sum of two independent SE-ARD kernels case B we observe that the fitted hyper parameters change dramatically. In each each SE-ARD kernel, one length scale parameter is significantly larger than the other. Therefore, that dimension has almost no effect on the covariance between points. This is similar to a covariance function that is the sum of two scalar squared exponential kernels in each input dimension. Essentially our GP can be decomposed as $f(x,y) = f_x(x) + f_y(y)$. 

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{E/kernel_comparison_1} 
        \subcaption{Case A}
        \label{fig:E_kernel_compare_1}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{E/kernel_comparison_2} 
        \subcaption{Case B}
        \label{fig:E_kernel_compare_2}
    \end{minipage}
    \caption{95\% Prediction intervals for models given in Table \ref{table:E_hyper_parameters} using data from cw1e.mat}
    \label{fig:E_kernel_compare}
\end{figure}

Both models have similar prediction intervals in the region of the data. However, the behaviour of the prediction intervals when extrapolating is different. While the first model very quickly returns to the prior prediction interval, the second model keeps smaller prediction intervals in directions parallel to the input axes. This is because in these directions, the "distance" measure to the data in one of the two SE-ARD kernels is small, as the corresponding ARD $\lambda$ is very large. This results in a decrease in the prediction interval from that kernel. 

The model in case B has a higher likelihood than the one in case A however we would argue that the case A model is better in most cases. By observing our dataset, we have not covered enough of the input space to conclude that we can decompose the function into a sum of functions in each input dimension. It would therefore not be appropriate to be confident in the extrapolation behaviour of this model.

\end{document}